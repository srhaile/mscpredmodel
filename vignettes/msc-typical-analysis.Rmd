---
title: "Multiple Score Comparison with `mscpredmodel` - a typical analysis workflow"
author: "Sarah R Haile, PhD"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Multiple Score Comparison with mscpredmodel - a typical analysis workflow}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  \usepackage[utf8]{inputenc}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Typical analysis

This is a basic example which shows you a typical analysis for a dataset having individual patient data from 10 cohorts, each with some combination of scores a, b, c, e, and g. 


```{r msc_setup}
options(digits = 4, scipen = 8)
library(mscpredmodel)
library(ggplot2)
theme_set(theme_bw())

dat <- msc_sample_data(seed = 12345678)
head(dat)
names(dat)[7:11] <- c("a", "b", "c", "d", "e")
dat$id <- 1:nrow(dat)

get_performance_statistics(data = dat, 
                          scores = c("a", "b", "c", "d", "e"), 
                          cohort = "study", outcome = "y", 
                          fn = list(AUC = c_statistic, 
                                    `O/E` = oe_ratio,
                                    BS = brier_score))

```

We want to examine performance of the 5 scores according to c-statistic (area under the curve, AUC), calibration slope (CS) and Brier score. 

```{r example1, message = FALSE, out.width = "80%"}
mod <- msc(scores = c("a", "b", "c", "d", "e"),
           outcome = "y", subjid = "id",
           cohort = "study", mods = NULL, data = dat,
           fn = list("AUC" = c_statistic,
                     "CS" = calibration_slope,
                     "BS" = brier_score),
           model = "consistency", direct = FALSE, indirect = FALSE,
           ref = "first")
mod
plot(mod)

check_homogeneity(mod)
```

We can rerun the model with moderators to check for transitivity.

```{r example2, out.width = "80%"}
modm <- msc(scores = c("a", "b", "c", "e", "g"),
           outcome = "outcome", subjid = "id",
           cohort = "study", 
           mods = c("age", "female", "x1"), 
           data = dat,
           fn = list("AUC" = c_statistic,
                     "CS" = calibration_slope,
                     "BS" = brier_score),
           model = "consistency", direct = FALSE, indirect = FALSE,
           ref = "first")
check_transitivity(modm, graph = TRUE)
```

Finally, if we want to compare direct, indirect and network evidence, we can also compute these comparisons.

```{r example3, eval = TRUE}
moddi <- msc(scores = c("a", "b", "c", "e", "g"),
           outcome = "outcome", subjid = "id",
           cohort = "study", mods = NULL, data = dat,
           fn = list("AUC" = c_statistic,
                     "CS" = calibration_slope,
                     "BS" = brier_score),
           model = "consistency", direct = TRUE, indirect = FALSE,
           ref = "all")
moddi

fit_msc(scores = c("a", "b", "c"),
        outcome = "outcome", subjid = "id",
        cohort = "study", mods = NULL, data = dat,
        perf_fn = c_statistic,
        perf_lbl = "c-statistic",
        direct = c("b", "c"), 
        indirect = NULL,
        model = "consistency")



#check_consistency(moddi, graph = TRUE)
```

