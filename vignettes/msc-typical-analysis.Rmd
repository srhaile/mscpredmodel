---
title: "Multiple Score Comparison with `mscpredmodel` - a typical analysis workflow"
author: "Sarah R Haile, PhD"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Multiple Score Comparison with mscpredmodel - a typical analysis workflow}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  \usepackage[utf8]{inputenc}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Typical analysis

This is a basic example which shows you a typical analysis for a dataset having individual patient data from 10 cohorts, each with some combination of scores a, b, c, e, and g. 


```{r msc_setup}
library(mscpredmodel)
library(ggplot2)
theme_set(theme_bw())

dat <- msc_sample_data(n.cohorts = 10)
head(dat)

get_performance_statistics(data = dat, 
                          scores = c("a", "b", "c", "e", "g"), 
                          cohort = "study", outcome = "outcome", 
                          fn = list(AUC = c_statistic, 
                                    `O/E` = oe_ratio,
                                    BS = brier_score))

```

We want to examine performance of the 5 scores according to c-statistic (area under the curve, AUC), calibration slope (CS) and Brier score. 

```{r example1}
mod <- msc(scores = c("a", "b", "c", "e", "g"),
           outcome = "outcome", subjid = "id",
           cohort = "study", mods = NULL, data = dat,
           fn = list("AUC" = c_statistic,
                     "CS" = calibration_slope,
                     "BS" = brier_score),
           model = "consistency", direct = FALSE, indirect = FALSE,
           ref = "first")
mod

check_homogeneity(mod)
```

We can rerun the model with moderators to check for transitivity.

```{r example2}
modm <- msc(scores = c("a", "b", "c", "e", "g"),
           outcome = "outcome", subjid = "id",
           cohort = "study", 
           mods = c("age", "female", "x1"), 
           data = dat,
           fn = list("AUC" = c_statistic,
                     "CS" = calibration_slope,
                     "BS" = brier_score),
           model = "consistency", direct = FALSE, indirect = FALSE,
           ref = "first")
check_transitivity(modm, graph = TRUE)
```

Finally, if we want to compare direct, indirect and network evidence, we can also compute these comparisons.

```{r example3, eval = FALSE}
moddi <- msc(scores = c("a", "b", "c", "e"),
           outcome = "outcome", subjid = "id",
           cohort = "study", mods = NULL, data = dat,
           fn = list("AUC" = c_statistic,
                     "CS" = calibration_slope,
                     "BS" = brier_score),
           model = "consistency", direct = TRUE, indirect = TRUE,
           ref = "first")

moddi

check_consistency(moddi, graph = TRUE)
```

