% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/msc.R
\name{msc}
\alias{msc}
\alias{print.msc}
\alias{plot.msc}
\alias{fit_msc}
\title{Full, direct and indirect network results for MSC}
\usage{
msc(
  scores = c("A", "B", "C", "D"),
  cohort = "cohort",
  outcome = "mort3",
  subjid = "id",
  fn = list(AUC = c_statistic, `O/E` = oe_ratio),
  mods = NULL,
  data = copddata,
  model = c("consistency", "inconsistency")[1],
  direct = TRUE,
  indirect = TRUE,
  ref = c("all", "first")[1],
  n.boot = 250,
  seed = NULL,
  max_missing = 0.8,
  ...
)

\method{print}{msc}(object)

\method{plot}{msc}(object)

fit_msc(
  scores = c("A", "B", "C", "D"),
  cohort = "cohort",
  outcome = "mort3",
  subjid = "id",
  perf_fn = c_statistic,
  perf_lbl = "c-statistic",
  direct = NULL,
  indirect = c("B", "C"),
  model = c("consistency", "inconsistency")[1],
  mods = NULL,
  data = this_dat,
  append_aggregate = NULL,
  n.boot = 250,
  seed = NULL,
  max_missing = 0.8,
  run_checks = TRUE,
  ...
)
}
\arguments{
\item{scores}{A vector of scores (i.e. variables in the dataset corresponding to calculated scores). The first score in the vector will be used as a comparator for all other scores.}

\item{cohort}{The name of the variable corresponding to cohort or study in the dataset.}

\item{outcome}{The name of the variable corresponding to the outcome.}

\item{subjid}{The name of the variable corresponding to the subject id.}

\item{fn}{A named list of performance metrics.}

\item{mods}{A vector list of potential moderators in the dataset.}

\item{data}{Name of dataset containing all other variables, with individual patient data, that is, 1 row per subject.}

\item{model}{Type of model (default "consistency", else "inconsistency"). It is sufficient to write \code{"c"} or \code{"i"}.}

\item{direct}{If a pair of scores are given, e.g. \code{c("B", "C")}, a direct estimate comparing those two scores is returned. (Default: NULL meaning full network estimates are computed.) In \code{msc_pairwise}, this should be given as TRUE or FALSE.}

\item{indirect}{If a pair of scores are given, e.g. \code{c("B", "C")}, a indirect estimate comparing those two scores is returned. (Default: NULL meaning full network estimates are computed.) If both \code{direct} and \code{indirect} are specified, only direct estimates are calculated. In \code{msc_pairwise}, this should be given as TRUE or FALSE.}

\item{ref}{Should all scores be compared to the \code{"first"} score, or \code{"all"} comparisons be calculated? The option \code{"all"} should be used if league tables are to be calculated afterwards. If code{ref = "all"}, one network model is run for each potential reference score, and direct / indirect models are run for all potential reference scores, rather than just all compared to the first score.}

\item{n.boot}{Number of bootstrap samples to be drawn. (Default: 250)}

\item{seed}{A random seed to make results reproducible (Default NULL)}

\item{max_missing}{If the proportion of missingness for any score within a cohort exceeds \code{max_missing}, the score will be counted as missing for the entire cohort. (Default 0.8)}

\item{...}{In the \code{msc} functions, any other arguments are passed to \code{\link[metafor]{rma.mv}}.}

\item{object}{An object of class \code{msc}, from \code{msc}.}

\item{append_aggregate}{Name of structured dataset of aggregate data for which no IPD is available. Requires the following variables: cohort, contr, design, score.1, score.2,   yi, vi. (Default: NULL) TO DO}
}
\value{
A list of class \code{msc}, with the following components:
\describe{
  \item{}{For each performance measure using the names from \code{fn}, a list containing \code{network}, \code{direct} if \code{direct = TRUE}, and \code{indirect} if \code{indirect = TRUE}. The coefficient tables from each \code{\link[metafor]{rma.mv}} model appear here}
  \item{data}{The dataset used in the network model.}
  \item{V}{The variance matrix used in the network model.}
  \item{rma.mv}{The full \code{\link[metafor]{rma.mv}} object for the network model.}
} 


The consistency model has random effects for contrast within cohort, while the inconsistency model has random effects for both contrast within cohort, and for contrast within design.

The main model in an analysis should probably not include any moderators, but they may be interesting when examining transitivity.
}
\description{
Compute comparisons of performance metrics for scores within a network.
}
\details{
The consistency and inconsistency models are those found in \href{https://doi.org/10.1186/s12874-016-0184-5}{Law et al 2016}: 

Specifically, we fit the models described in Law et al 2016, which differ only in their random effects:
\describe{
  \item{consistency}{First item} random contrast within study
  \item{inconsistency}{Second item} random contrast within study, and random contrast within design
}
}
\section{Functions}{
\itemize{
\item \code{msc()}: Compute MSC models for various performance measures, returning only the model results

\item \code{fit_msc()}: Compute MSC models for various performance measures, returning only the model results

}}
\examples{
dat <- msc_sample_data()
out1 <- msc(scores = c("a", "b", "c", "d", "e"),
         cohort = "study", outcome = "outcome", subjid = "id", 
         data = dat, direct = FALSE, indirect = FALSE,
         model = "inconsistency")
print(out1)

\dontrun{
 library(ggplot2)
 plot(out1)
}

}
\seealso{
Law, M.; Jackson, D.; Turner, R.; Rhodes, K. & Viechtbauer, W. Two new methods to fit models for network meta-analysis with random inconsistency effects BMC Medical Research Methodology, 2016, 16, 87.#'
}
